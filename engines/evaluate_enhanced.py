import os\nimport argparse\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Dict, List\nimport json\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport models\nimport loaders\nfrom core.adaptive_multiScale_interaction import EnhancedMambaInterpreter\nfrom core.causal_state_intervention import EnhancedCausalMambaFramework\nfrom core.temporal_state_evolution import EnhancedTemporalMambaAnalyzer\nfrom core.multimodal_enhancement import EnhancedMultiModalMambaFramework\nfrom core.constraints import StateConstraint\n\n\nclass ComprehensiveEvaluator:\n    \"\"\"\n    Comprehensive evaluation framework for enhanced Vision-Mamba-Mender\n    \"\"\"\n    \n    def __init__(self, args):\n        self.args = args\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Initialize model and components\n        self._init_model()\n        self._init_data_loader()\n        self._init_analysis_components()\n        \n        # Results storage\n        self.evaluation_results = {\n            'classification_metrics': {},\n            'interpretability_analysis': {},\n            'causal_analysis': {},\n            'temporal_analysis': {},\n            'multimodal_analysis': {},\n            'comparative_analysis': {}\n        }\n    \n    def _init_model(self):\n        \"\"\"Initialize and load the trained model\"\"\"\n        self.model = models.load_model(\n            self.args.model_name,\n            num_classes=self.args.num_classes\n        )\n        \n        # Load trained weights\n        checkpoint = torch.load(self.args.model_path, map_location=self.device)\n        if 'model_state_dict' in checkpoint:\n            self.model.load_state_dict(checkpoint['model_state_dict'])\n        else:\n            self.model.load_state_dict(checkpoint)\n            \n        self.model.to(self.device)\n        self.model.eval()\n    \n    def _init_data_loader(self):\n        \"\"\"Initialize test data loader\"\"\"\n        self.test_loader = loaders.load_data(\n            self.args.data_dir,\n            self.args.data_name,\n            data_type='test',\n            batch_size=self.args.batch_size,\n            args=self.args\n        )\n    \n    def _init_analysis_components(self):\n        \"\"\"Initialize analysis components\"\"\"\n        # Original constraint (if applicable)\n        self.original_constraint = None\n        if hasattr(self.args, 'constraint_config'):\n            self.original_constraint = StateConstraint(\n                model=self.model,\n                model_name=self.args.model_name,\n                **self.args.constraint_config\n            )\n        \n        # Novel analysis components\n        self.adaptive_interpreter = EnhancedMambaInterpreter(\n            self.model, self.original_constraint)\n        \n        self.causal_framework = EnhancedCausalMambaFramework(\n            self.model, self.original_constraint)\n        \n        self.temporal_analyzer = EnhancedTemporalMambaAnalyzer(\n            self.model, self.original_constraint)\n        \n        if getattr(self.args, 'enable_multimodal', False):\n            self.multimodal_framework = EnhancedMultiModalMambaFramework(\n                self.model, self.original_constraint)\n        else:\n            self.multimodal_framework = None\n    \n    def evaluate_classification_performance(self) -> Dict:\n        \"\"\"Evaluate basic classification performance\"\"\"\n        print(\"Evaluating classification performance...\")\n        \n        all_predictions = []\n        all_labels = []\n        all_confidences = []\n        \n        with torch.no_grad():\n            for batch_idx, samples in enumerate(self.test_loader):\n                inputs, labels, _ = samples\n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n                \n                outputs = self.model(inputs)\n                probabilities = torch.softmax(outputs, dim=1)\n                predictions = torch.argmax(outputs, dim=1)\n                confidences = torch.max(probabilities, dim=1)[0]\n                \n                all_predictions.extend(predictions.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                all_confidences.extend(confidences.cpu().numpy())\n        \n        # Compute metrics\n        accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n        \n        # Classification report\n        class_names = [f'Class_{i}' for i in range(self.args.num_classes)]\n        class_report = classification_report(\n            all_labels, all_predictions, \n            target_names=class_names, \n            output_dict=True\n        )\n        \n        # Confusion matrix\n        conf_matrix = confusion_matrix(all_labels, all_predictions)\n        \n        results = {\n            'accuracy': accuracy,\n            'predictions': all_predictions,\n            'labels': all_labels,\n            'confidences': all_confidences,\n            'classification_report': class_report,\n            'confusion_matrix': conf_matrix\n        }\n        \n        self.evaluation_results['classification_metrics'] = results\n        print(f\"Classification Accuracy: {accuracy:.4f}\")\n        \n        return results\n    \n    def evaluate_interpretability(self, num_samples: int = 100) -> Dict:\n        \"\"\"Evaluate interpretability using adaptive multi-scale analysis\"\"\"\n        print(\"Evaluating interpretability with adaptive multi-scale analysis...\")\n        \n        interpretability_results = []\n        sample_count = 0\n        \n        with torch.no_grad():\n            for batch_idx, samples in enumerate(self.test_loader):\n                if sample_count >= num_samples:\n                    break\n                    \n                inputs, labels, _ = samples\n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n                \n                outputs = self.model(inputs)\n                \n                # Adaptive multi-scale analysis\n                adaptive_results = self.adaptive_interpreter.enhanced_loss_computation(\n                    outputs, labels, inputs, epoch=0, total_epochs=1)\n                \n                interpretability_results.append({\n                    'batch_idx': batch_idx,\n                    'complexity': adaptive_results['amil_results']['complexity'].mean().item(),\n                    'adaptive_loss': adaptive_results['adaptive_loss'].item(),\n                    'selected_layers': adaptive_results['amil_results']['selected_layers'],\n                    'layer_importance': adaptive_results['amil_results']['layer_importance'].mean(dim=0).cpu().numpy()\n                })\n                \n                sample_count += inputs.size(0)\n        \n        # Aggregate results\n        avg_complexity = np.mean([r['complexity'] for r in interpretability_results])\n        avg_adaptive_loss = np.mean([r['adaptive_loss'] for r in interpretability_results])\n        \n        # Layer importance aggregation\n        all_importance = np.array([r['layer_importance'] for r in interpretability_results])\n        avg_layer_importance = np.mean(all_importance, axis=0)\n        \n        results = {\n            'average_complexity': avg_complexity,\n            'average_adaptive_loss': avg_adaptive_loss,\n            'layer_importance_distribution': avg_layer_importance,\n            'detailed_results': interpretability_results\n        }\n        \n        self.evaluation_results['interpretability_analysis'] = results\n        print(f\"Average Input Complexity: {avg_complexity:.4f}\")\n        print(f\"Average Adaptive Loss: {avg_adaptive_loss:.6f}\")\n        \n        return results\n    \n    def evaluate_causal_mechanisms(self, num_samples: int = 50) -> Dict:\n        \"\"\"Evaluate causal intervention mechanisms\"\"\"\n        print(\"Evaluating causal intervention mechanisms...\")\n        \n        causal_results = []\n        sample_count = 0\n        \n        with torch.no_grad():\n            for batch_idx, samples in enumerate(self.test_loader):\n                if sample_count >= num_samples:\n                    break\n                    \n                inputs, labels, _ = samples\n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n                \n                # Causal analysis (reduced scope for efficiency)\n                causal_analysis = self.causal_framework.analyze_causal_mechanisms(\n                    inputs, labels, num_analysis_steps=3)\n                \n                causal_results.append({\n                    'batch_idx': batch_idx,\n                    'causal_strengths': causal_analysis['causal_strengths'],\n                    'importance_scores': causal_analysis['importance_scores']\n                })\n                \n                sample_count += inputs.size(0)\n        \n        # Aggregate causal strength statistics\n        all_strengths = []\n        all_importance = []\n        \n        for result in causal_results:\n            all_strengths.extend(list(result['causal_strengths'].values()))\n            all_importance.extend(list(result['importance_scores'].values()))\n        \n        results = {\n            'average_causal_strength': np.mean(all_strengths) if all_strengths else 0,\n            'causal_strength_std': np.std(all_strengths) if all_strengths else 0,\n            'average_importance': np.mean(all_importance) if all_importance else 0,\n            'importance_std': np.std(all_importance) if all_importance else 0,\n            'detailed_results': causal_results\n        }\n        \n        self.evaluation_results['causal_analysis'] = results\n        print(f\"Average Causal Strength: {results['average_causal_strength']:.4f}\")\n        \n        return results\n    \n    def evaluate_temporal_evolution(self, num_samples: int = 30) -> Dict:\n        \"\"\"Evaluate temporal state evolution patterns\"\"\"\n        print(\"Evaluating temporal state evolution...\")\n        \n        temporal_results = []\n        sample_count = 0\n        \n        with torch.no_grad():\n            for batch_idx, samples in enumerate(self.test_loader):\n                if sample_count >= num_samples:\n                    break\n                    \n                inputs, labels, _ = samples\n                inputs = inputs.to(self.device)\n                \n                # Temporal analysis\n                temporal_analysis = self.temporal_analyzer.comprehensive_temporal_analysis(\n                    inputs, save_dir=None)\n                \n                temporal_results.append({\n                    'batch_idx': batch_idx,\n                    'temporal_metrics': temporal_analysis['temporal_metrics']\n                })\n                \n                sample_count += inputs.size(0)\n        \n        # Aggregate temporal metrics\n        all_metrics = {}\n        for metric_name in ['stability', 'complexity', 'smoothness', 'information_flow']:\n            values = [r['temporal_metrics'].get(metric_name, 0) for r in temporal_results]\n            all_metrics[metric_name] = {\n                'mean': np.mean(values),\n                'std': np.std(values),\n                'values': values\n            }\n        \n        results = {\n            'aggregated_metrics': all_metrics,\n            'detailed_results': temporal_results\n        }\n        \n        self.evaluation_results['temporal_analysis'] = results\n        print(f\"Average Stability: {all_metrics['stability']['mean']:.4f}\")\n        print(f\"Average Smoothness: {all_metrics['smoothness']['mean']:.4f}\")\n        \n        return results\n    \n    def generate_comparative_analysis(self) -> Dict:\n        \"\"\"Generate comparative analysis between novel and baseline methods\"\"\"\n        print(\"Generating comparative analysis...\")\n        \n        # Compare with baseline (model without novel components)\n        baseline_accuracy = self.evaluation_results['classification_metrics']['accuracy']\n        \n        # Compute improvement metrics\n        interpretability_score = 1.0 - self.evaluation_results['interpretability_analysis']['average_adaptive_loss']\n        causal_reliability = 1.0 / (1.0 + self.evaluation_results['causal_analysis']['causal_strength_std'])\n        temporal_quality = self.evaluation_results['temporal_analysis']['aggregated_metrics']['stability']['mean']\n        \n        # Overall enhancement score\n        enhancement_score = (interpretability_score + causal_reliability + temporal_quality) / 3\n        \n        results = {\n            'baseline_accuracy': baseline_accuracy,\n            'interpretability_score': interpretability_score,\n            'causal_reliability': causal_reliability,\n            'temporal_quality': temporal_quality,\n            'overall_enhancement_score': enhancement_score,\n            'improvement_breakdown': {\n                'interpretability': interpretability_score * 100,\n                'causal_analysis': causal_reliability * 100,\n                'temporal_stability': temporal_quality * 100\n            }\n        }\n        \n        self.evaluation_results['comparative_analysis'] = results\n        print(f\"Overall Enhancement Score: {enhancement_score:.4f}\")\n        \n        return results\n    \n    def visualize_results(self, save_dir: str):\n        \"\"\"Generate comprehensive visualizations\"\"\"\n        print(f\"Generating visualizations in {save_dir}...\")\n        \n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        \n        # 1. Classification performance visualization\n        self._plot_classification_results(save_dir)\n        \n        # 2. Interpretability analysis visualization\n        self._plot_interpretability_analysis(save_dir)\n        \n        # 3. Causal analysis visualization\n        self._plot_causal_analysis(save_dir)\n        \n        # 4. Temporal evolution visualization\n        self._plot_temporal_analysis(save_dir)\n        \n        # 5. Comparative analysis\n        self._plot_comparative_analysis(save_dir)\n    \n    def _plot_classification_results(self, save_dir: str):\n        \"\"\"Plot classification performance results\"\"\"\n        results = self.evaluation_results['classification_metrics']\n        \n        # Confusion matrix\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n        plt.title('Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'), dpi=300)\n        plt.close()\n        \n        # Confidence distribution\n        plt.figure(figsize=(10, 6))\n        plt.hist(results['confidences'], bins=50, alpha=0.7, edgecolor='black')\n        plt.title('Prediction Confidence Distribution')\n        plt.xlabel('Confidence Score')\n        plt.ylabel('Frequency')\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, 'confidence_distribution.png'), dpi=300)\n        plt.close()\n    \n    def _plot_interpretability_analysis(self, save_dir: str):\n        \"\"\"Plot interpretability analysis results\"\"\"\n        results = self.evaluation_results['interpretability_analysis']\n        \n        # Layer importance distribution\n        plt.figure(figsize=(12, 6))\n        layer_indices = range(len(results['layer_importance_distribution']))\n        plt.bar(layer_indices, results['layer_importance_distribution'])\n        plt.title('Layer Importance Distribution')\n        plt.xlabel('Layer Index')\n        plt.ylabel('Importance Score')\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, 'layer_importance.png'), dpi=300)\n        plt.close()\n        \n        # Complexity vs Adaptive Loss\n        detailed_results = results['detailed_results']\n        complexities = [r['complexity'] for r in detailed_results]\n        adaptive_losses = [r['adaptive_loss'] for r in detailed_results]\n        \n        plt.figure(figsize=(10, 6))\n        plt.scatter(complexities, adaptive_losses, alpha=0.6)\n        plt.xlabel('Input Complexity')\n        plt.ylabel('Adaptive Loss')\n        plt.title('Complexity vs Adaptive Loss')\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, 'complexity_vs_loss.png'), dpi=300)\n        plt.close()\n    \n    def _plot_causal_analysis(self, save_dir: str):\n        \"\"\"Plot causal analysis results\"\"\"\n        results = self.evaluation_results['causal_analysis']\n        \n        # Causal strength statistics\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Bar plot of average values\n        metrics = ['Causal Strength', 'Importance Score']\n        values = [results['average_causal_strength'], results['average_importance']]\n        errors = [results['causal_strength_std'], results['importance_std']]\n        \n        ax1.bar(metrics, values, yerr=errors, capsize=5, alpha=0.7)\n        ax1.set_title('Average Causal Metrics')\n        ax1.set_ylabel('Score')\n        ax1.grid(True, alpha=0.3)\n        \n        # Distribution of causal strengths\n        detailed_results = results['detailed_results']\n        all_strengths = []\n        for result in detailed_results:\n            all_strengths.extend(list(result['causal_strengths'].values()))\n        \n        if all_strengths:\n            ax2.hist(all_strengths, bins=30, alpha=0.7, edgecolor='black')\n            ax2.set_title('Causal Strength Distribution')\n            ax2.set_xlabel('Causal Strength')\n            ax2.set_ylabel('Frequency')\n            ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, 'causal_analysis.png'), dpi=300)\n        plt.close()\n    \n    def _plot_temporal_analysis(self, save_dir: str):\n        \"\"\"Plot temporal evolution analysis results\"\"\"\n        results = self.evaluation_results['temporal_analysis']\n        metrics = results['aggregated_metrics']\n        \n        # Temporal metrics comparison\n        plt.figure(figsize=(12, 8))\n        \n        metric_names = list(metrics.keys())\n        means = [metrics[name]['mean'] for name in metric_names]\n        stds = [metrics[name]['std'] for name in metric_names]\n        \n        x_pos = np.arange(len(metric_names))\n        \n        plt.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7,\n                color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n        plt.xlabel('Temporal Metrics')\n        plt.ylabel('Score')\n        plt.title('Temporal Evolution Quality Metrics')\n        plt.xticks(x_pos, metric_names, rotation=45)\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, 'temporal_metrics.png'), dpi=300)\n        plt.close()\n    \n    def _plot_comparative_analysis(self, save_dir: str):\n        \"\"\"Plot comparative analysis results\"\"\"\n        results = self.evaluation_results['comparative_analysis']\n        \n        # Enhancement breakdown\n        plt.figure(figsize=(10, 6))\n        \n        improvements = results['improvement_breakdown']\n        categories = list(improvements.keys())\n        values = list(improvements.values())\n        \n        colors = ['skyblue', 'lightcoral', 'lightgreen']\n        bars = plt.bar(categories, values, color=colors, alpha=0.7, edgecolor='black')\n        \n        # Add value labels on bars\n        for bar, value in zip(bars, values):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                    f'{value:.1f}%', ha='center', va='bottom')\n        \n        plt.title('Enhancement Components Performance')\n        plt.ylabel('Improvement Score (%)')\n        plt.ylim(0, max(values) * 1.2)\n        plt.xticks(rotation=45)\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, 'enhancement_breakdown.png'), dpi=300)\n        plt.close()\n    \n    def save_results(self, save_path: str):\n        \"\"\"Save all evaluation results to JSON\"\"\"\n        print(f\"Saving results to {save_path}...\")\n        \n        # Convert numpy arrays to lists for JSON serialization\n        serializable_results = self._make_serializable(self.evaluation_results)\n        \n        with open(save_path, 'w') as f:\n            json.dump(serializable_results, f, indent=2)\n    \n    def _make_serializable(self, data):\n        \"\"\"Convert numpy arrays and other non-serializable objects\"\"\"\n        if isinstance(data, np.ndarray):\n            return data.tolist()\n        elif isinstance(data, np.integer):\n            return int(data)\n        elif isinstance(data, np.floating):\n            return float(data)\n        elif isinstance(data, dict):\n            return {k: self._make_serializable(v) for k, v in data.items()}\n        elif isinstance(data, list):\n            return [self._make_serializable(item) for item in data]\n        else:\n            return data\n    \n    def run_comprehensive_evaluation(self):\n        \"\"\"Run complete evaluation pipeline\"\"\"\n        print(\"Starting comprehensive evaluation of enhanced Vision-Mamba-Mender...\")\n        print(\"=\"*60)\n        \n        # 1. Classification performance\n        self.evaluate_classification_performance()\n        \n        # 2. Interpretability analysis\n        self.evaluate_interpretability()\n        \n        # 3. Causal mechanism evaluation\n        self.evaluate_causal_mechanisms()\n        \n        # 4. Temporal evolution analysis\n        self.evaluate_temporal_evolution()\n        \n        # 5. Comparative analysis\n        self.generate_comparative_analysis()\n        \n        # 6. Generate visualizations\n        viz_dir = os.path.join(os.path.dirname(self.args.output_dir), 'visualizations')\n        self.visualize_results(viz_dir)\n        \n        # 7. Save results\n        results_path = os.path.join(self.args.output_dir, 'comprehensive_evaluation_results.json')\n        self.save_results(results_path)\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"Evaluation completed successfully!\")\n        print(f\"Results saved to: {results_path}\")\n        print(f\"Visualizations saved to: {viz_dir}\")\n        print(\"=\"*60)\n        \n        return self.evaluation_results\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Comprehensive Evaluation of Enhanced Vision-Mamba-Mender')\n    \n    # Model and data arguments\n    parser.add_argument('--model_name', default='vmamba_tiny', type=str, help='model name')\n    parser.add_argument('--model_path', required=True, type=str, help='path to trained model')\n    parser.add_argument('--data_name', default='imagenet', type=str, help='dataset name')\n    parser.add_argument('--data_dir', required=True, type=str, help='test data directory')\n    parser.add_argument('--num_classes', default=1000, type=int, help='number of classes')\n    parser.add_argument('--batch_size', default=32, type=int, help='batch size')\n    \n    # Output arguments\n    parser.add_argument('--output_dir', default='./evaluation_results', type=str, help='output directory')\n    \n    # Analysis configuration\n    parser.add_argument('--enable_multimodal', action='store_true', help='enable multi-modal analysis')\n    \n    args = parser.parse_args()\n    \n    # Create output directory\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    \n    # Run evaluation\n    evaluator = ComprehensiveEvaluator(args)\n    results = evaluator.run_comprehensive_evaluation()\n    \n    # Print summary\n    print(\"\\nEvaluation Summary:\")\n    print(\"-\" * 30)\n    print(f\"Classification Accuracy: {results['classification_metrics']['accuracy']:.4f}\")\n    print(f\"Interpretability Score: {results['interpretability_analysis']['average_complexity']:.4f}\")\n    print(f\"Causal Reliability: {results['causal_analysis']['average_causal_strength']:.4f}\")\n    print(f\"Temporal Quality: {results['temporal_analysis']['aggregated_metrics']['stability']['mean']:.4f}\")\n    print(f\"Overall Enhancement: {results['comparative_analysis']['overall_enhancement_score']:.4f}\")\n\n\nif __name__ == '__main__':\n    main()"
